# Assistente Virtual UEA - Desafio LLM/RAG

Este projeto consiste em um sistema de **RAG (Retrieval-Augmented Generation)** desenvolvido para o processo seletivo do Projeto ADA. O objetivo √© responder perguntas sobre documentos institucionais da Universidade do Estado do Amazonas  utilizando um modelo de linguagem (LLM) rodando localmente.

## üèóÔ∏è Arquitetura Geral

O sistema √© composto por tr√™s m√≥dulos principais:

1.  **Pipeline de Ingest√£o (`src/ingest.py`)**:
    * L√™ os PDFs armazenados na pasta `data/pdfs/`.
    * Extrai o texto utilizando a biblioteca **PyMuPDF (fitz)**.
    * Divide o texto em *chunks* de 500 caracteres (com sobreposi√ß√£o de 50 caracteres).
    * Gera embeddings utilizando o modelo **SentenceTransformer** (`all-MiniLM-L6-v2`).
    * Indexa e armazena os vetores utilizando o **FAISS** (CPU) para recupera√ß√£o eficiente.

2.  **Pipeline RAG (`src/rag.py`)**:
    * Carrega o √≠ndice FAISS e os chunks processados.
    * Utiliza o modelo de linguagem **TinyLlama-1.1B-Chat-v1.0** (quantizado/otimizado para execu√ß√£o local via Hugging Face Transformers).
    * Ao receber uma pergunta, converte-a em embedding, recupera os 3 trechos mais relevantes e constr√≥i um prompt enriquecido com esse contexto.
    * Gera a resposta final baseada estritamente no contexto recuperado.

3.  **API HTTP (`api/main.py`)**:
    * Desenvolvida com **FastAPI**.
    * Exp√µe o endpoint `/ask` para intera√ß√£o com o usu√°rio.

---

## üöÄ Como Rodar o Projeto

### Op√ß√£o 1: Via Docker (Recomendado)

O projeto est√° totalmente "dockerizado" para facilitar a execu√ß√£o. O comando de execu√ß√£o do container j√° realiza a ingest√£o dos documentos e sobe a API.

1.  **Construir a imagem:**
    ```bash
    docker build -t assistente-uea .
    ```

2.  **Rodar o container:**
    ```bash
    docker run -p 8000:8000 assistente-uea
    ```
    *Aguarde alguns instantes enquanto o script realiza a ingest√£o dos PDFs e carrega o modelo LLM na mem√≥ria.*

### Op√ß√£o 2: Execu√ß√£o Local

Caso prefira rodar fora do Docker, siga os passos:

1.  **Instalar depend√™ncias:**
    Recomenda-se usar um ambiente virtual (venv).
    ```bash
    pip install -r requirements.txt
    ```

2.  **Gerar o √çndice (Ingest√£o):**
    Execute o script para processar os PDFs e criar o banco vetorial.
    ```bash
    python src/ingest.py
    ```

3.  **Iniciar a API:**
    ```bash
    uvicorn api.main:app --host 0.0.0.0 --port 8000
    ```

---

## üì° Como Chamar a API

A API estar√° dispon√≠vel em `http://localhost:8000`.
Acesse: `http://localhost:8000/docs`

### Endpoint: `POST /ask`

Recebe uma pergunta e retorna a resposta gerada pelo LLM.

* **Exemplo de Requisi√ß√£o (cURL):**
    ```bash
    curl -X POST "http://localhost:8000/ask" \
         -H "Content-Type: application/json" \
         -d '{"question": "Quais s√£o os requisitos para a Casa do Estudante?"}'
    ```

* **Corpo da Requisi√ß√£o (JSON):**
    ```json
    {
      "question": "Quais s√£o os requisitos para a Casa do Estudante?"
    }
    ```

* **Exemplo de Resposta:**
    ```json
    {
      "answer": "Para concorrer a uma vaga, o aluno deve estar regularmente matriculado..."
    }
    ```

### Endpoint: `GET /health`

Verifica se a API est√° online.

---

## ‚úÖ Funcionalidades Implementadas

Conforme os requisitos do desafio, as seguintes funcionalidades foram implementadas:

* [x] **Pipeline de Ingest√£o de Documentos**: Script automatizado para leitura de PDFs, chunking e indexa√ß√£o vetorial.
* [x] **Execu√ß√£o de Modelo LLM Local**: Uso do `TinyLlama-1.1B` rodando em CPU via biblioteca `transformers`.
* [x] **Pipeline de RAG**: Integra√ß√£o entre busca vetorial (FAISS) e gera√ß√£o de texto para respostas contextualizadas.
* [x] **API HTTP (FastAPI)**: Endpoint funcional para consulta.
* [x] **Docker**: `Dockerfile` otimizado configurado para instalar depend√™ncias e executar a aplica√ß√£o.

---

## Observa√ß√µes e Limita√ß√µes

Este projeto utiliza o modelo **TinyLlama-1.1B**, que √© um *Small Language Model* (SLM) otimizado para rodar em ambientes com recursos limitados (como CPUs comuns). Embora eficiente para o prop√≥sito deste desafio, ele possui limita√ß√µes inerentes:

1.  **Alucina√ß√µes (Hallucinations)**: Devido ao seu tamanho reduzido de par√¢metros (1.1B), o modelo pode ocasionalmente gerar respostas que n√£o est√£o totalmente alinhadas com o contexto fornecido ou inventar informa√ß√µes quando a resposta n√£o √© encontrada nos documentos.
2.  **Seguimento de Instru√ß√µes**: A capacidade de seguir instru√ß√µes complexas no prompt √© inferior a modelos maiores (como Llama-3-8B ou GPT-4). Em alguns casos, ele pode ser verboso ou repetir partes do texto.
3.  **Desempenho em CPU**: Embora leve, a infer√™ncia em CPU √© naturalmente mais lenta do que em GPU. O tempo de resposta pode variar dependendo do hardware onde o Docker est√° sendo executado.
4.  **Janela de Contexto**: O modelo possui uma janela de contexto limitada. Se os trechos recuperados pelo RAG forem muito longos, informa√ß√µes do in√≠cio do prompt podem ser "esquecidas" ou truncadas.
